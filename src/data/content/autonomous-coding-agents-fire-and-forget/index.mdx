---
title: 'Autonomous Coding Agents: Fire and Forget'
contentType: 'essay'
description: 'Exploring the promise and perils of fully autonomous AI coding agents'
author: 'Jack Ivers'
publishDate: '2025-11-11T16:57:45-06:00'
image: '~/assets/images/content/autonomous-coding-agents-fire-and-forget/cruise_missiles.png'
featured: false
draft: true
tags: ['ai', 'coding-agents', 'developer-tools']
categories: ['ai-and-technology']
---

import SingleImage from '~/components/ui/SingleImage.astro';
import ImageGallery from '~/components/ui/ImageGallery.astro';
import ChatTranscript from '~/components/ui/ChatTranscript.astro';
import MDContent from '~/components/ui/MDContent.astro';
import { YouTube } from 'astro-embed';
import { terminologyDiscussion } from './transcripts/terminology-discussion';

## Introduction

This [post from Simon Willison](https://simonwillison.net/2025/Nov/6/async-code-research/) appeared on my radar yesterday morning and it struck a chord with me. AI coding tools are a research sweet spot for me--that is, I try very hard to stay up--and I've been working the new cloud-based tools from OpenAI and Anthropic into my workflows as much as possible.

<SingleImage
  src="willison_code_research.png"
  alt="Simon Willison's code research post"
  size="2xl"
  postDir="autonomous-coding-agents-fire-and-forget"
/>

## What to Call These Things?

Willison uses the term "asynchronous coding agents" for the tool category exempified by OpenAI's Codex Cloud and Anthropicâ€™s Claude Code on the web. I didn't like the mouth-feel of that name, and decided to check in with a smart friend of mine, Opus 4.1, who had a solid understanding of the concept and how its being described. Opus and I prefer autonomous to asynchronous, so: "autonomous coding agents."

<ChatTranscript transcript={terminologyDiscussion} theme="adium" maxHeight="600px" />

## Background: The Coding Wars

Anthropic has invested heavily in the AI coding space. Its langauge models surprised many of us by jumping into the coding-model lead in June 2024 with the release of Claude 3.5 Sonnet, and marched ahead with follow-on releases including Sonnet 4.5 just over a month ago. I have used Anthropic coding models almost exclusively since 3.5 Sonnet arrived.

Earlier this year, Anthropic jumped deeper into the fray with an actual coding tool, the minimalist coding agent Claude Code whose motto might be "get out of the way and let our great models do their thing." Not long after Claude Code arrived, it supplanted Cursor[^1] as my primary AI coding interface.

So where was OpenAI, the biggest AI player of all? Since 3.5 Sonnet arrived, playing catch-up. I had a lot of respect for OpenAI models generally, for ChatGPT as a product, and for the robustness of their APIs and inference services. But coding belonged to Anthropic.

## Sea Change: Autonomy

OpenAI wasn't, by any measure, standing still. Their release of GPT-5 in August was huge, clearly a substantial step forward, and some were saying it rivaled Claude models for coding. OpenAI followed on up with a coding-tuned model, GPT-5 Codex, in mid-September. OpenAI also released its own minimalist coding agent, Codex CLI. But to me, this all felt like catch-up: just we-try-to-do-everything posturing from OpenAI.

What caused me to reconsider was an ongoing stream of podcast and YouTube interviews where someone from OpenAI (including president Greg Brockman) would be talking about some topics, and there'd be an aside where they talked about how OpenAI was dogfooding Codex internally to build OpenAI's own products. Here's an example from OpenAI DevDay on October 7th, where Sherwin Wu and Christina Cai of the OpenAI Platform Team get onto the subject of Codex:

<YouTube id="ImBGzWS1qd0" title="OpenAI DevDay: Codex Platform Team Discussion" />

What jumped out at me was how OpenAI's internal teams were using Codex, heavily, in autonomous agent mode; fire-and-forget. They told stories about going into meetings, and being asked to "give me a couple minutes here ..." so people could fire off a few Codex Cloud cruise missles that would grind away during the meeting, and have results waiting with them afterwards.

Experiencing a series of legit, unstaged anecdotes like this---I'm pretty good at smelling "marketing message" and these definitely weren't---my attitude about Codex went from "meh" to "NEED TO INSTALL ASAP." Before I got Codex Cloud installed, Anthropic launched their version, "Claude Code on the Web" (it's hilarious how bad both OpenAI and Anthropic are with product names). Since I was double deep with Claude Code already, getting its web counterpart set up was low effort, and [CCOTW](https://simonwillison.net/2025/Oct/20/claude-code-for-web/) ended up being my first Autonomous Coding Agent experience. Codex Cloud followed soon after.

## Fire-and-forget That Works

In Claude Code the coding agent, you can already fire-and-forget. It's called YOLO mode. You turn off all protections, and let Claude do whatever it chooses to, which in a command line or terminal interface, is pretty much anything. Nuke the system drive? Sure thing. Wipe the GitHub repo? Yep. Shut down your AWS cloud? No problem. Claude is a good AI-person, better than most, but YOLO mode is dangerous and developers understand the need to use it carefully.

So we mostly do our important work in non-YOLO mode, which is a lot of carefully watching what the agent is doing in real time, and way too much respodning to the agent's polite requests "can I use this dangerous tool, please?" I get a lot of code written when I pair with Claude Code, but while doing so I'm close to 100% occupied keeping close tabs on what Claude is up to.

<SingleImage
  src="cruise_missiles.png"
  alt="Codex Cloud cruise missiles"
  size="2xl"
  postDir="autonomous-coding-agents-fire-and-forget"
/>

What OpenAI and Anthropic really did with ACAs is give us (fairly) safe YOLO mode by equiping their agents with reasonably secure environments that the agent can go to town in, without risking much. "Go to town" means "run whatever you want, safely" -- usually called sandboxing -- but also "code whatever you want, safely -- you can't break anything." This works because the agent does its work in its own personal code space, kept safely separate from the current codebase and other work-in-process. Nothing there impacts our real code, until we've had a chance to carefully look at it and let it in (to be merged).

## Well-equipped Environments, Well-defined Specs

Willison's piece has pulled me the rest of the way into Autonomous Coding Agents; I did the research and set up my first custom Codex Cloud environment just now. Until now, I've been living with Codex's default setup with no customization and conservative default protections.

The point of Willison's post is really using autonomous coding agents for coding _research projects_ and he walks us through several real-life examples. I like his suggestion to have a separate Git repo for this kind of research---one more layer of "don't worry, you can't break anything."

The human work in this autonomous new world? Clear, well-thought-out specs that precisely communicate what the agent should build for you. There's nothing preventing you from having 10 or 20 agents working away---except 10 or 20 good, clear specs.

---

[^1]: Well, not exactly. My current insane, wonderful dev setup looks like this: container app: Cursor; left pane: Codex CLI; center pane: source files; right pane: Claude Code

<SingleImage
  src="crazy_panes.png"
  alt="Crazy panes dev setup"
  size="2xl"
  postDir="autonomous-coding-agents-fire-and-forget"
/>
