---
title: 'AI Vision vs ML OCR: Are the Curves Crossing?'
contentType: 'essay'
description: ''
author: 'Jack Ivers'
publishDate: '2025-12-01T12:56:34-0600'
image: '~/assets/images/content/ai-vision-vs-ml-ocr-are-the-curves-crossing/'
featured: false
featuredOrder: 10
draft: true
tags: []
categories: []
---

import SingleImage from '~/components/ui/SingleImage.astro';
import ImageGallery from '~/components/ui/ImageGallery.astro';
import ChatTranscript from '~/components/ui/ChatTranscript.astro';
import MDContent from '~/components/ui/MDContent.astro';

{/* Post content goes here */}

I thought AI Vision had won out over "old style OCR" when I wrote [this piece](chatgpt-4v) back in February 2024. It turns out I was wrong; but maybe not for long.

## Wrong Assumptions

I have a huge knowledge gap, and that is what bit me. I had some significant experience with traditional OCR, many years ago, and it was mostly bad. I then completely skipped the Computer Vision era. I had heard of Computer Vision, but assumed that it was just a rebranding of old, bad, OCR.

My next significant experience with extracting content from images, therefore, came on the abovementioned learning project where I applied OpenAI's GPT-4V dedicated vision model to extract and structure often-handwritten data. That effort was amazingly successful. That led me to decide, back in early 2024, that OCR (rebranded or not) was dead, long live AI Vision.

Wrong.

## Sorry, Guy on Hacker News

That was still my attitude as I kicked off a new AI Vision project earlier this year. The proof of concept phase mostly went well; handwriting recognition worked very well, and the only real extraction problem I had was checkbox detection. I was able to prompt my way to checkbox success, and coming into the production implementation felt quite confident.

Right around this point, I saw a comment on a [Hacker News post announcing the new Qwen3-VL next-gen vision model](https://news.ycombinator.com/item?id=45354408) where the writer was saying he had really bad luck using OpenAI vision models on a particular construction form. He went on to say that Qwen3-VL did a great job on it.

I replied with a kind of classic, "you're holding it wrong" response. Sorry, `richardlblair`, I now know you were holding it just fine.

## My Comeuppance

So we shifted into real implementation on the vision project, and there was a pre-printed form, which customers would print out, then complete by hand. Its primary data section, was a matrix of checkboxes. Roughly 10 rows by 15 columns.

This checkbox matrix proved to be the undoing of AI Vision, at least for this particular task. This was the GPT-5, Gemini Pro 2.5 timeframe, not long ago in human time. None of the mainstream models could reliably detect checkboxes in that dense matrix of 150 cells. This "simple preprinted form" seemed about to derail our implementation.

## A Fresh Savior?

That Hacker News article caught my eye for more reasons than one. Qwen3-VL was family of a special-purpose vision model of various sizes and features, freshly released within a few days of my dilemma becoming apparent. We had an OpenRouter path already implemented, so I wired up Qwen3-vl-32B, and after a bit of configuring and prompt tweaking it looked like we had a solution: checkmark detection and selected cell identification was nearly perfect. I was a little nervous that only a few inference providers offered Qwen3-vl models; but hey, it was working extremely well.

Until, one day, it just stopped working. I can't say with 100% confidence that our code didn't cause the problem. I **_can_** say I'm 98+% sure, however, since we actually rolled the code back to its exact state when checkbox detection last worked reliably---and that code no longer detected checkboxes reliably. If I had to guess, I'd say the model was prone to entering a corrupt state, losing its mind in effect.

The downside of working with a bleeding-edge model, served by 3rd party inference providers, over OpenRouter and its extra layer of API translation---who ya gonna call? Ghostbusters would have been a better answer by far. If your OpenAI or Anthropic model has issues, at the very least you know they have people closely following the health of the model and deeply concerned if anything looks broken. With the long, weak chain of custody in our OpenRouter configuration? I'm not sure anyone was paying attention at any of the parties. I did post a ticket to OpenRouter, with the only visible result being that they shut down the endpoint I was using, completely and without notice.

We switched to a larger Qwen3-VL model and continued trying to make things work, to no avail.

In fairness, based on my genuine early success with the model, I believe that a bugfixed, properly configured, correctly operated Qwen3 VL model would be capable of checkmark detection and selected cell identification. Unfortunately, our early-days experience was more the "buggy and unreliable" flavor.

## Computer Vision? I Don't Wanna ...

My next step was actually to quickly prototype a classical Python CV solution using readily-available CV libraries. This immediately exposed a world of hurt with no clear escape; our use case wasn't 100,000 forms of the exact same size and shape, and it wasn't clear how to even get the CV engine focused on the right part of the form, when that part would almost never be in the same place from one image to the next. Not a fair trial by any means, but I think my insticts were correct in abandoning classic Python CV as a possible solution.

My partner on the project suggested we check out Amazon Textract. I was skeptical, of course, certainly not any less so based on my classical CV attempt. But AWS has a nice UI for quickly testing Textract, and it didn't take long to be won over, as form after inconsistent form came through with good handwriting extraction and virtually perfect checkmark detection.

Textract is a thing I didn't know existed: a machine learning augmented CV service. It applies ML to overcome hard CV problems (like checkbox detection!) and reduce the learning curve and implementation pain. In caveman speak, the flow is

> Here Textract, take image. Sideways? Upside-down? Not me! You fix!! Give back form data! Give back table data!

And there you go, done! It's ML, very well executed.

## ML Is Not AI ...

Following the tremendous relief of finding a reliable form parsing solution, I began to notice that Textract, while it solved our most crucial problem, was in other areas less effective than AI. Textract's ML was powerful but it was not a language model and was clearly very dumb about language. AI would continually surprise and delight, realizing that handwriting that looks like "Co" was actually part of an address, and that the more correct extraction would be "CO" the state abbreviation. Or see a series of handwritten numbers "3427" ... "28" ... "29" ... "30" and realize these should be extracted as 3427, 3428, 3429, 3430. Many small differences but they added up.

We decided to try taking Textract's raw form and table data, and passing it through the AI with a simple prompt, basically "refine this." Magical!

results were less good than AI's, though in less critical areas. Textract just did its job, giving us
the
As I've admitted, I'm not a world's leading authority on Computer Vision. But my understand of what

o didn't break something, as

Just as AIs had their period of not being able to do simple math, I learned that some visual elements were beyond current state of the art, general-purpose but vision-enabled AI models' ability to understand.

, extremely successfully, ferom heirloom recipe cards. hubbzero experience in
ap
