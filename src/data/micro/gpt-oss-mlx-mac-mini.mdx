---
title: "Running OpenAI's gpt-oss on Mac Mini with MLX"
description: "Thanks to LM Studio and the community, it's insanely easy. And gpt-oss is insanely good."
author: "Jack Ivers"
publishDate: "2025-08-06T08:00:00-05:00"
image: "~/assets/images/blog/gpt-oss-mlx-mac-mini/gpt_oss.png"
draft: false
---

import SingleImage from '~/components/ui/SingleImage.astro';

OpenAI dropped their open source, [Apache-licensed](https://www.apache.org/licenses/LICENSE-2.0) ["gpt-oss" models](https://openai.com/open-models/) yesterday, which obviously is getting a lot of attention.

I've experimented with local models over the years, mainly to assess "how smart is it?" and "how badly does it eat up local resources?" Generally they weren't really something I'd use in anger when I had access to much more powerful OpenAI and [Anthropic](https://www.anthropic.com/) models.

I hadn't done a local LLM install for quite a while though, not since I got my M4 Mac Mini with 64GB of unified CPU/GPU memory ... and gpt-oss is a full reasoning model, like OpenAI's [o3](https://openai.com/index/openai-o3), so it seemed like the time was right to do another local model eval.

Several wows here:

- Wow---it is so easy to get local models running these days. [LM Studio](https://lmstudio.ai/) is amazing.
- Wow---the "local models" community is incredible. Within 24 hours, someone had a fully-optimized-for-Mac version of gpt-oss (using Apple's [MLX](https://github.com/ml-explore/mlx) framework) was ready and waiting for download in LM Studio.
- Wow---gpt-oss is a for-real powerful model, not a toy like I'd experimented with a year or two back.

I had a few research questions floating around that I hadn't had a chance to ask o3 about yet, so just for grins I decided to throw them at gpt-oss. I got a strong, useful, reasoning-grade responses, such as this complex topic relating to the [Hugo](https://gohugo.io/) static site generator:

<SingleImage src="gpt_oss_mac_mini.png" alt="gpt_oss_mac_mini.png" size="2xl" postDir="gpt-oss-mlx-mac-mini" />

Verdict: good enough to actually use.