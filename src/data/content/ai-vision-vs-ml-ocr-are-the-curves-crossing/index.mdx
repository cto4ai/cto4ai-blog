---
title: 'AI Vision vs ML OCR: Are the Curves Crossing?'
contentType: 'essay'
description: ''
author: 'Jack Ivers'
publishDate: '2025-12-01T12:56:34-0600'
image: '~/assets/images/content/ai-vision-vs-ml-ocr-are-the-curves-crossing/'
featured: false
featuredOrder: 10
draft: true
tags: []
categories: []
---

import SingleImage from '~/components/ui/SingleImage.astro';
import ImageGallery from '~/components/ui/ImageGallery.astro';
import ChatTranscript from '~/components/ui/ChatTranscript.astro';
import MDContent from '~/components/ui/MDContent.astro';

{/* Post content goes here */}

I thought AI Vision had won out over "old style OCR" when I wrote [this piece](chatgpt-4v) back in February 2024. It turns out I was wrong; but maybe not for long.

## Wrong Assumptions

I have a huge knowledge gap, and that is what bit me. I had some significant experience with traditional OCR, many years ago, and it was mostly bad. I then completely skipped the Computer Vision era. I had heard of Computer Vision, but assumed that it was just a rebranding of old, bad, OCR.

My next significant experience with extracting content from images, therefore, came on the abovementioned learning project where I applied OpenAI's GPT-4V dedicated vision model to extract and structure often-handwritten data. That effort was amazingly successful. That led me to decide, back in early 2024, that OCR (rebranded or not) was dead, long live AI Vision.

Wrong.

## Sorry, Guy on Hacker News

That was still my attitude as I kicked off a new AI Vision project earlier this year. The proof of concept phase mostly went well; handwriting recognition worked very well, and the only real extraction problem I had was checkbox detection. I was able to prompt my way to checkbox success, and coming into the production implementation felt quite confident.

Right around this point, I saw a comment on a [Hacker News post announcing the new Qwen3-VL next-gen vision model](https://news.ycombinator.com/item?id=45354408) where the writer was saying he had really bad luck using OpenAI vision models on a particular construction form. He went on to say that Qwen3-VL did a great job on it.

I replied with a kind of classic, "you're holding it wrong" response. Sorry, `richardlblair`, I now know you were holding it just fine.

## My Comeuppance

So we shifted into real implementation on the vision project, and there was a pre-printed form, which customers would print out, then complete by hand. Its primary data section, was a matrix of checkboxes. Roughly 10 rows by 15 columns.

This checkbox matrix proved to be the undoing of AI Vision, at least for this particular task. This was the GPT-5, Gemini Pro 2.5 timeframe, not long ago in human time. None of the mainstream models could reliably detect checkboxes in that dense matrix of 150 cells. This "simple preprinted form" seemed about to derail our implementation.

## A Fresh Savior?

The Hacker News article caught my eye for more reasons than one. Qwen3-VL was family of a special-purpose vision model of various sizes and features, freshly released within a few days of my dilemma becoming apparent. We had an OpenRouter path already implemented, so I wired up Qwen3-vl-32B, and after a bit of configuring and prompt tweaking it looked like we had a solution: checkmark detection and selected cell identification was nearly perfect. I was a little nervous that

Just as AIs had their period of not being able to do simple math, I learned that some visual elements were beyond current state of the art, general-purpose but vision-enabled AI models' ability to understand.

, extremely successfully, ferom heirloom recipe cards. hubbzero experience in
ap
